# -*- coding: utf-8 -*-
"""Predict the Introverts from the Extroverts.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d3KqIwTQgw0p9OSVJyJGng44-q_dJ_gI

# Kaggle Playground Series - Season 5 Episode 7 (July 2025)
###  Personality Prediction: Introvert vs Extrovert

**Author:** Magdum Shaikh  
**Competition Link:** [Kaggle Competition](https://www.kaggle.com/competitions/playground-series-s5e7)  
**Date:** July 2025

---

##  Problem Statement

In this competition, we aim to classify whether an individual is an **Introvert** or an **Extrovert** based on their personality traits and social behaviors. The dataset includes features like:
- Time spent alone
- Frequency of attending social events
- Social media posting frequency
- Drained feelings after socializing
- Friends circle size

This is a binary classification task evaluated on **Accuracy Score**.

---

##  Dataset Overview

The dataset includes:
- **`train.csv`**: Contains labeled data with `Personality` column as target.
- **`test.csv`**: Test data with no target values.
- **`sample_submission.csv`**: Format for final predictions.

| Feature Name              | Description                          |
|--------------------------|--------------------------------------|
| `id`                     | Unique identifier                    |
| `Time_spent_Alone`       | Minutes person spends alone daily    |
| `Stage_fear`             | Numerical score of public speaking fear |
| `Social_event_attendance` | Frequency of attending social events |
| `Going_outside`          | Outdoor activity frequency           |
| `Drained_after_socializing` | Score representing exhaustion after socializing |
| `Friends_circle_size`    | Count of close friends               |
| `Post_frequency`         | Frequency of social media posting    |
| `Personality`            | **Target**: Introvert or Extrovert   |

---

##  Our Approach

1. **Data Exploration & Cleaning**
   - Understand missing values and fix them
   - Explore class balance, distributions

2. **EDA (Exploratory Data Analysis)**
   - Visualize correlations
   - Compare feature patterns between introverts vs extroverts

3. **Feature Engineering**
   - Standardize/normalize if required
   - Create meaningful derived features if helpful

4. **Modeling**
   - Use ML algorithms like:
     - Logistic Regression
     - Random Forest
     - XGBoost / LightGBM
   - Tune hyperparameters with GridSearchCV or Optuna

5. **Evaluation**
   - Use validation set to check accuracy
   - Try stratified K-Fold for stability

6. **Submission**
   - Generate predictions
   - Format submission file as per `sample_submission.csv`
   - Submit on Kaggle

---

##  Final Thoughts

This competition is a great chance to practice tabular data analysis, binary classification, and feature engineering. The use of synthetic data ensures fairness and allows us to freely explore different techniques.

I hope this notebook helps beginners understand how to approach real ML problems using step-by-step methods.

---

##  Goal

- [ ] Achieve > 90% Accuracy
- [ ] Learn from top public notebooks
- [ ] Improve feature engineering and modeling techniques
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier

"""###loading Data from kaggle"""

from google.colab import drive
drive.mount('/content/drive')

!mkdir -p ~/.kaggle
!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d sbhatti34/predict-the-introverts-from-the-extroverts

!unzip predict-the-introverts-from-the-extroverts.zip -d data/

train_df = pd.read_csv('/content/data/train.csv')
test_df = pd.read_csv('/content/data/test.csv')

train_df.head(10)

train_df.tail()

train_df.shape

train_df.info()

train_df.describe()

train_df.isnull().sum()

numerical_column = train_df.select_dtypes(exclude = ['object']).columns
categorical_column = train_df.select_dtypes(include = ['object']).columns

numerical_column

categorical_column

"""####visualizing target column"""

target_counts = train_df['Personality'].value_counts()
print(target_counts)


sns.countplot(data=train_df, x='Personality')
plt.title("Distribution of Personality Classes")
plt.show()

"""####Stage_fear, Drained_after_socializing these two columns are present in Yes or No categorical data so converting it into numeric data by 1 and 0"""

test_df.head()

# Exclude id and target
features = [col for col in train_df.columns if col not in ['id', 'Personality']]
features = [col for col in test_df.columns if col not in ['id']]

yes_no_col = ['Stage_fear','Drained_after_socializing']
for col in yes_no_col:
  train_df[col] = train_df[col].map({'Yes': 1, 'No':0})

yes_no_col = ['Stage_fear','Drained_after_socializing']
for col in yes_no_col:
  test_df[col] = test_df[col].map({'Yes': 1, 'No':0})

train_df.head()

"""#### Plot distribution of each numerical feature"""

import seaborn as sns
import matplotlib.pyplot as plt

for feature in features:
    plt.figure(figsize=(9, 6))
    sns.histplot(train_df[feature], kde=True, bins=30)
    plt.title(f'Distribution of {feature}')
    plt.tight_layout()
    plt.show()

"""####filling missing values"""

train_df['Time_spent_Alone'].fillna(train_df['Time_spent_Alone'].median(), inplace=True)
train_df['Stage_fear'].fillna(train_df['Stage_fear'].mean(), inplace=True)
train_df['Social_event_attendance'].fillna(train_df['Social_event_attendance'].median(), inplace=True)
train_df['Going_outside'].fillna(train_df['Going_outside'].mean(), inplace=True)
train_df['Drained_after_socializing'].fillna(train_df['Drained_after_socializing'].median(), inplace=True)
train_df['Friends_circle_size'].fillna(train_df['Friends_circle_size'].median(), inplace=True)
train_df['Post_frequency'].fillna(train_df['Post_frequency'].mode()[0], inplace=True)

test_df['Time_spent_Alone'].fillna(train_df['Time_spent_Alone'].median(), inplace=True)
test_df['Stage_fear'].fillna(train_df['Stage_fear'].mean(), inplace=True)
test_df['Social_event_attendance'].fillna(train_df['Social_event_attendance'].median(), inplace=True)
test_df['Going_outside'].fillna(train_df['Going_outside'].mean(), inplace=True)
test_df['Drained_after_socializing'].fillna(train_df['Drained_after_socializing'].median(), inplace=True)
test_df['Friends_circle_size'].fillna(train_df['Friends_circle_size'].median(), inplace=True)
test_df['Post_frequency'].fillna(train_df['Post_frequency'].mode()[0], inplace=True)

"""####creating new features"""

train_df.head(2)

scaler = MinMaxScaler()
cols_to_scale = ['Social_event_attendance', 'Friends_circle_size', 'Time_spent_Alone', 'Drained_after_socializing']
train_df[cols_to_scale] = scaler.fit_transform(train_df[cols_to_scale])
test_df[cols_to_scale] = scaler.fit_transform(test_df[cols_to_scale])

train_df['social_activity_score'] = train_df['Social_event_attendance'] + train_df['Friends_circle_size']
train_df['introversion_score'] = train_df['Time_spent_Alone'] + train_df['Drained_after_socializing']
train_df['social_balance_score'] = train_df['social_activity_score'] - train_df['introversion_score']

test_df['social_activity_score'] =test_df['Social_event_attendance'] +test_df['Friends_circle_size']
test_df['introversion_score'] =   test_df['Time_spent_Alone']       + test_df['Drained_after_socializing']
test_df['social_balance_score'] = test_df['social_activity_score'] -  test_df['introversion_score']

# Compute correlation matrix
corr_matrix = train_df.corr(numeric_only=True)

# Plot heatmap
plt.figure(figsize=(9, 5))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap')
plt.show()

train_df.drop(['Stage_fear', 'social_balance_score'], axis=1, inplace=True)
test_df.drop(['Stage_fear', 'social_balance_score'], axis=1, inplace=True)

le = LabelEncoder()
train_df['Personality'] = le.fit_transform(train_df['Personality'])

"""###Model training"""

# -----------------------------------
# üéØ 3. Split features and labels
# -----------------------------------
X_train = train_df.drop('Personality', axis=1)
y_train = train_df['Personality']
X_test = test_df.copy()

# -----------------------------------
# üìè 4. Scaling
# -----------------------------------
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# -----------------------------------
# ü§ñ 5. Model Training
# -----------------------------------
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train_scaled, y_train)

svm = SVC(kernel='rbf', probability=True)
svm.fit(X_train_scaled, y_train)

xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
xgb.fit(X_train_scaled, y_train)

# -----------------------------------
# üîÆ 6. Prediction (on test_df without labels)
# -----------------------------------
rf_preds = rf.predict(X_test_scaled)
svm_preds = svm.predict(X_test_scaled)
xgb_preds = xgb.predict(X_test_scaled)

rf_preds_decoded = le.inverse_transform(rf_preds)
svm_preds_decoded = le.inverse_transform(svm_preds)
xgb_preds_decoded = le.inverse_transform(xgb_preds)

# Choose the best one or all:
test_df['Predicted_Personality_RF'] = rf_preds_decoded
test_df['Predicted_Personality_SVM'] = svm_preds_decoded
test_df['Predicted_Personality_XGB'] = xgb_preds_decoded



# ‚úÖ Final result
print(test_df[['Predicted_Personality_RF', 'Predicted_Personality_SVM', 'Predicted_Personality_XGB']].head())

# test_df[['id', 'Predicted_Personality_RF']].to_csv("submission .csv", index=False)

final_submission = test_df[['id']].copy()
final_submission['Personality'] = test_df['Predicted_Personality_XGB']

# Save to CSV
final_submission.to_csv("submission.csv", index=False)

